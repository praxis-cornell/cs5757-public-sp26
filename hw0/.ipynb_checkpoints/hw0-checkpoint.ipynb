{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bfd0c88",
   "metadata": {},
   "source": [
    "## 0.4. Least-Squares Via JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13edd831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autograder'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjnp\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01monp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mautograder\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msolution\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     14\u001b[39m     get_problem_data,\n\u001b[32m     15\u001b[39m     least_squares_loss,\n\u001b[32m     16\u001b[39m     lstsq_params_closed_form,\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m     22\u001b[39m data = onp.loadtxt(\u001b[33m\"\u001b[39m\u001b[33mdata/motor_data_large.csv\u001b[39m\u001b[33m\"\u001b[39m, delimiter=\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m, skiprows=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'autograder'"
     ]
    }
   ],
   "source": [
    "# JAX + plotting niceties\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as onp\n",
    "\n",
    "from solution import (\n",
    "    get_problem_data,\n",
    "    least_squares_loss,\n",
    "    lstsq_params_closed_form,\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Load data\n",
    "# -----------------------\n",
    "data = onp.loadtxt(\"data/motor_data_large.csv\", delimiter=\",\", skiprows=1)\n",
    "current = jnp.asarray(data[:, 0])\n",
    "torque = jnp.asarray(data[:, 1])\n",
    "\n",
    "# -----------------------\n",
    "# Normalize (fit in normalized space)\n",
    "# -----------------------\n",
    "eps = 1e-12\n",
    "\n",
    "current_mean = jnp.mean(current)\n",
    "current_std  = jnp.std(current) + eps\n",
    "\n",
    "torque_mean = jnp.mean(torque)\n",
    "torque_std  = jnp.std(torque) + eps\n",
    "\n",
    "current_n = (current - current_mean) / current_std\n",
    "torque_n  = (torque  - torque_mean)  / torque_std\n",
    "\n",
    "A, y = get_problem_data(current_n, torque_n)\n",
    "\n",
    "def unnormalize_params(\n",
    "    params_n: jnp.ndarray,\n",
    "    current_mean: jnp.ndarray,\n",
    "    current_std: jnp.ndarray,\n",
    "    torque_mean: jnp.ndarray,\n",
    "    torque_std: jnp.ndarray,\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"Convert quadratic model parameters back to raw units.\n",
    "\n",
    "    Normalized model:\n",
    "        torque_n = a_n + b_n * current_n + c_n * current_n^2\n",
    "\n",
    "    Returns parameters [a, b, c] such that:\n",
    "        torque ≈ a + b * current + c * current^2\n",
    "    \"\"\"\n",
    "    a_n, b_n, c_n = params_n\n",
    "\n",
    "    inv_std = 1.0 / current_std\n",
    "    inv_std2 = inv_std * inv_std\n",
    "\n",
    "    c = torque_std * c_n * inv_std2\n",
    "    b = torque_std * (b_n * inv_std - 2.0 * c_n * current_mean * inv_std2)\n",
    "    a = torque_mean + torque_std * (\n",
    "        a_n\n",
    "        - b_n * current_mean * inv_std\n",
    "        + c_n * current_mean**2 * inv_std2\n",
    "    )\n",
    "\n",
    "    return jnp.array([a, b, c])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd442450",
   "metadata": {},
   "source": [
    "## Gradient descent in JAX\n",
    "\n",
    "Here we'll implement basic gradient descent loop using two core JAX primitives and the `least_squares_loss` implemented in `solutions.py`:\n",
    "\n",
    "- `jax.grad` to compute gradients\n",
    "- `jax.lax.scan` to run an iterative update inside JIT-compiled code\n",
    "\n",
    "Our goal is to show how to use JAX's autodiff and JIT compilation features to efficiently solve optimization problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c51c7a",
   "metadata": {},
   "source": [
    "### One gradient descent step\n",
    "\n",
    "We first write the loss as a pure function of the parameters and data.  \n",
    "Given this loss, a single gradient descent step is:\n",
    "$$\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta \\ell(\\theta)$$\n",
    "where $\\alpha. > 0$ is the learning rate.\n",
    "\n",
    "\n",
    "In JAX, this is implemented by differentiating the loss with `jax.grad`\n",
    "and applying the update directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864beb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_step(x: jnp.ndarray, A: jnp.ndarray, y: jnp.ndarray, lr: float) -> jnp.ndarray:\n",
    "    g = jax.grad(least_squares_loss)(x, A, y)\n",
    "    return x - lr * g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee749e2",
   "metadata": {},
   "source": [
    "### Running the update loop with `jax.lax.scan`\n",
    "\n",
    "Normally we would write gradient descent with a `for` loop in Python:\n",
    "\n",
    "    for t in range(T):\n",
    "        g = jax.grad(least_squares_loss)(params, A, y)\n",
    "        params = params - lr * g\n",
    "        losses.append(least_squares_loss(params, A, y))\n",
    "\n",
    "But, in JAX, Python `for` loops do not compose well with `jax.jit`. Instead, we'll use\n",
    "`jax.lax.scan`, which is a JAX-friendly way to express loops that can be JIT-compiled.\n",
    "\n",
    "When using `jax.lax.scan`, each iteration of the loop has:\n",
    "\n",
    "- a **state** carried from step to step (here: the parameters)\n",
    "- an optional **output** recorded at each step (here: the loss)\n",
    "\n",
    "Conceptually, the update looks like:\n",
    "\n",
    "    params₀ → params₁ → params₂ → … → params_T\n",
    "                 ↓        ↓            ↓\n",
    "               loss₀    loss₁         loss_T\n",
    "\n",
    "Using `scan` lets us run the entire optimization loop inside JAX’s compiled\n",
    "computation graph, rather than unrolling it in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaee392",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnames=[\"num_steps\"])\n",
    "def run_gd(A: jnp.ndarray, y: jnp.ndarray, x0: jnp.ndarray, lr: float, num_steps: int) -> tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    def body(x: jnp.ndarray, _: None) -> tuple[jnp.ndarray, jnp.ndarray]:\n",
    "        loss = least_squares_loss(x, A, y)\n",
    "        x = grad_step(x, A, y, lr)\n",
    "        return x, loss\n",
    "\n",
    "    x_final, losses = jax.lax.scan(body, x0, xs=None, length=num_steps)\n",
    "    return x_final, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904602dd",
   "metadata": {},
   "source": [
    "### Comparing gradient descent to the closed-form solution\n",
    "\n",
    "We run gradient descent from an initial guess and compare the final loss\n",
    "to the analytic least-squares solution. Both are computed in normalized\n",
    "space; later we convert the parameters back to the original units for\n",
    "interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47fb872",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "num_steps = 500\n",
    "x0 = jnp.zeros((3,))\n",
    "\n",
    "x_gd, losses = run_gd(A, y, x0, lr=lr, num_steps=num_steps)\n",
    "\n",
    "loss_final_gd = least_squares_loss(x_gd, A, y)\n",
    "\n",
    "# Compare gradient descent to closed-form solution.\n",
    "x_star = lstsq_params_closed_form(A, y)  \n",
    "loss_star = least_squares_loss(x_star, A, y)\n",
    "\n",
    "print(\"normalized-space losses:\")\n",
    "print(\"  GD final loss:\", float(loss_final_gd))\n",
    "print(\"  closed-form loss:\", float(loss_star))\n",
    "\n",
    "x_star_unnorm = unnormalize_params(x_star, current_mean, current_std, torque_mean, torque_std)\n",
    "x_gd_unnorm   = unnormalize_params(x_gd, current_mean, current_std, torque_mean, torque_std)\n",
    "\n",
    "print(\"\\noriginal-space params (for tau vs I):\")\n",
    "print(\"  closed-form:\", onp.array(x_star_unnorm))\n",
    "print(\"  GD:\",         onp.array(x_gd_unnorm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8c17cd",
   "metadata": {},
   "source": [
    "### Plotting and analysis\n",
    "\n",
    "Finally, we plot the loss over iterations to visualize convergence,\n",
    "and visualize the model fit against the data from from both gradient descent and\n",
    "the closed-form solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f123d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Plot: loss curve + unnormalized data with both fits\n",
    "# -----------------------\n",
    "losses_np = onp.asarray(losses)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses_np)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"loss (normalized)\")\n",
    "plt.title(\"Gradient descent loss curve\")\n",
    "plt.tight_layout()\n",
    "\n",
    "def predict(x_unnorm: jnp.ndarray, I_vals: jnp.ndarray) -> jnp.ndarray:\n",
    "    a, b, c = x_unnorm\n",
    "    return a + b * I_vals + c * (I_vals**2)\n",
    "\n",
    "I_plot = onp.asarray(current)\n",
    "tau_plot = onp.asarray(torque)\n",
    "\n",
    "I_min, I_max = float(current.min()), float(current.max())\n",
    "I_grid = jnp.linspace(I_min, I_max, 400)\n",
    "\n",
    "tau_hat_star = predict(x_star_unnorm, I_grid)\n",
    "tau_hat_gd   = predict(x_gd_unnorm,   I_grid)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(I_plot, tau_plot, s=8, alpha=0.6, label=\"data\")\n",
    "plt.plot(onp.asarray(I_grid), \n",
    "         onp.asarray(tau_hat_star), \n",
    "         linewidth=2, \n",
    "         c=\"C1\", \n",
    "         label=\"analytic (closed-form)\")\n",
    "plt.plot(onp.asarray(I_grid), \n",
    "         onp.asarray(tau_hat_gd),   \n",
    "         linewidth=2, \n",
    "         c=\"C2\", \n",
    "         label=\"gradient descent\")\n",
    "plt.xlabel(\"I (A)\")\n",
    "plt.ylabel(\"Torque (N*m)\")\n",
    "plt.title(\"Data vs fitted curves\")\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
